{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.python.ops.variables import trainable_variables\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "#import data\n",
    "(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = np.array(x_train,np.float32)\n",
    "x_test = np.array(x_test,np.float32)\n",
    "\n",
    "#dataset param\n",
    "num_classes = 10\n",
    "num_features = 784\n",
    "\n",
    "#training param\n",
    "learning_rate = 0.001 \n",
    "training_step  =1000\n",
    "batch_size = 32\n",
    "display_step = 100\n",
    "\n",
    "\n",
    "num_inputs =28\n",
    "timesteps=28\n",
    "num_units = 32\n",
    "\n",
    "#tf.data\n",
    "data_train = tensorflow.data.Dataset.from_tensor_slices(\n",
    " (x_train, y_train)\n",
    ")\n",
    "data_train=  data_train.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model,layers\n",
    "class LSTM(Model):\n",
    " def __init__(self):\n",
    "  super(LSTM,self).__init__()\n",
    "  self.lstm_layer=layers.LSTM(units=num_units)\n",
    "  self.out = layers.Dense(num_classes)\n",
    " def call(self,x,is_training=False):\n",
    "  x=self.lstm_layer(x)\n",
    "  x=self.out(x)\n",
    "  if not is_training:\n",
    "   x= tf.nn.softmax(x)\n",
    "  return x\n",
    "lstm_net=LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 15:56:58.497167: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [60000,28,28]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-11 15:56:58.497503: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [60000,28,28]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-11 15:57:02.654580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-05-11 15:57:03.037521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-11 15:57:03.539861: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x5585d06f7950 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-11 15:57:03.540073: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2023-05-11 15:57:03.611328: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-11 15:57:04.164715: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f200135fbe0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "step100 loss:2.2381744384765625 acc:0.40625\n",
      "step200 loss:2.1200454235076904 acc:0.5625\n",
      "step300 loss:2.0533645153045654 acc:0.5625\n",
      "step400 loss:1.9543893337249756 acc:0.71875\n",
      "step500 loss:1.953502893447876 acc:0.625\n",
      "step600 loss:1.9263888597488403 acc:0.6875\n",
      "step700 loss:1.9477057456970215 acc:0.71875\n",
      "step800 loss:1.8767294883728027 acc:0.65625\n",
      "step900 loss:1.837165117263794 acc:0.8125\n",
      "step1000 loss:1.8022446632385254 acc:0.84375\n"
     ]
    }
   ],
   "source": [
    "#compute loss\n",
    "def cross_entropy(x,y):\n",
    " y = tf.cast(y,tf.int64)\n",
    " loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=x)\n",
    " return tf.reduce_mean(loss)\n",
    "\n",
    "#accuracy function\n",
    "def accuracy(y_pred,y_true):\n",
    " correct_prediction = tf.equal(tf.argmax(y_pred,1),tf.cast(y_true,tf.int64))\n",
    " return tf.reduce_mean(tf.cast(correct_prediction,tf.float32),axis=-1)\n",
    "\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "def run_optimization(x,y):\n",
    " with tf.GradientTape() as g:\n",
    "  pred =lstm_net(x,is_training=True)\n",
    "  loss = cross_entropy(pred,y)\n",
    "  trainable_variables = lstm_net.trainable_variables\n",
    " gradients = g.gradient(loss,trainable_variables)\n",
    " optimizer.apply_gradients(zip(gradients,trainable_variables))\n",
    " \n",
    " \n",
    "for step, (batch_x,batch_y) in enumerate(data_train.take(training_step),1):\n",
    "  run_optimization(batch_x,batch_y)\n",
    "  if step % display_step ==0:\n",
    "   pred = lstm_net(batch_x)\n",
    "   loss = cross_entropy(pred,batch_y)\n",
    "   acc = accuracy(pred,batch_y)\n",
    "   print(f\"step{step} loss:{loss} acc:{acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
